{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8253337",
   "metadata": {},
   "source": [
    "# Model 3: Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78090e0",
   "metadata": {},
   "source": [
    "The purpose of this model is to utilize reinforcement learning to improve the prediction capabilities of Model 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7158c66d",
   "metadata": {},
   "source": [
    "## Preliminary Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206f4e42",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca028f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import random\n",
    "import nbimporter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a57b42",
   "metadata": {},
   "source": [
    "### ACTOR Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dc8bc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_features):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_features, 128)\n",
    "        self.fc2 = nn.Linear(128, 5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x)) # Use ReLU\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba50df8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Actor(nn.Module):\n",
    "    \n",
    "#     def __init__(self, num_features):\n",
    "#         super(Actor, self).__init__()\n",
    "#         self.fc1 = nn.Linear(num_features, 128)\n",
    "#         self.fc2 = nn.Linear(128, 5)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "        \n",
    "#         return F.softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a231ab",
   "metadata": {},
   "source": [
    "### ACTOR Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08b7c8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_actor(model, optimizer, X_train, y_train, epochs):\n",
    "    loss_list = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad() # Resets gradients\n",
    "        outputs = model(X_train) # Forward Propagation\n",
    "        loss = F.cross_entropy(outputs, y_train) # Calculate loss using cross-entropy\n",
    "        loss.backward() # Backward Propagation\n",
    "        optimizer.step() # Updates params\n",
    "        \n",
    "        loss_list.append(loss.item())\n",
    "        \n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item()}\")\n",
    "        \n",
    "    return loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d8228d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_actor(model, X_train, y_train, X_test, y_test):\n",
    "    with torch.no_grad():\n",
    "        train_outputs = model(X_train)\n",
    "        _, train_predicted = torch.max(train_outputs, 1)\n",
    "        \n",
    "        test_outputs = model(X_test)\n",
    "        _, test_predicted = torch.max(test_outputs, 1)\n",
    "\n",
    "    # Calculate train and test accuracy\n",
    "    train_accuracy = accuracy_score(y_train, train_predicted)\n",
    "    print(f'Train Accuracy: {train_accuracy}')\n",
    "    \n",
    "    test_accuracy = accuracy_score(y_test, test_predicted)\n",
    "    print(f'Test Accuracy: {test_accuracy}')\n",
    "\n",
    "    return train_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a893c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_action(actor, X_input, valid):\n",
    "    with torch.no_grad():\n",
    "        output = actor(X_input)\n",
    "        \n",
    "    valid_tensor = torch.tensor(valid, dtype = torch.bool)\n",
    "    valid_output = torch.where(valid_tensor, output, torch.tensor(float('-inf')).to(output.dtype))\n",
    "    \n",
    "    random_choice = random.randint(1, 5)\n",
    "    \n",
    "    if random_choice == 1:\n",
    "        _, predicted_action = torch.where(valid_output != float('-inf'))\n",
    "        return np.random.choice(predicted_action).item()\n",
    "    else:\n",
    "        _, predicted_action = torch.max(valid_output, 1)\n",
    "        return predicted_action.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf98842b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(loss_list):\n",
    "    plt.plot(loss_list)\n",
    "    plt.title('Training Loss per Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8226512d",
   "metadata": {},
   "source": [
    "### CRITIC Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7812dfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_features, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))  # Use ReLU\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe29d29",
   "metadata": {},
   "source": [
    "### CRITIC Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d632d8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_critic(model, optimizer, X_train, y_train, epochs):\n",
    "    loss_list = []\n",
    "    \n",
    "    y_train_float = y_train.float()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()  # Resets gradients\n",
    "        outputs = model(X_train).squeeze() # Forward Propagation\n",
    "        loss = F.binary_cross_entropy(outputs, y_train_float)  # Binary Cross-Entropy Loss\n",
    "        loss.backward()  # Backward Propagation\n",
    "        optimizer.step()  # Updates params\n",
    "        \n",
    "        loss_list.append(loss.item())\n",
    "        \n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item()}\")\n",
    "        \n",
    "    return loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c6491be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_critic(model, X_train, y_train, X_test, y_test):\n",
    "    with torch.no_grad():\n",
    "        train_outputs = model(X_train).squeeze()\n",
    "        test_outputs = model(X_test).squeeze()\n",
    "        \n",
    "        # Threshold set to 0.5\n",
    "        train_predicted = (train_outputs > 0.5).float()\n",
    "        test_predicted = (test_outputs > 0.5).float()\n",
    "\n",
    "    # Calculate train and test accuracy\n",
    "    train_accuracy = accuracy_score(y_train, train_predicted)\n",
    "    print(f'Train Accuracy: {train_accuracy}')\n",
    "    \n",
    "    test_accuracy = accuracy_score(y_test, test_predicted)\n",
    "    print(f'Test Accuracy: {test_accuracy}')\n",
    "\n",
    "    return train_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5c1e5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_prob_success(critic, X_input):\n",
    "    with torch.no_grad():\n",
    "        prob_success = critic(X_input)\n",
    "        \n",
    "        return prob_success.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cae62b",
   "metadata": {},
   "source": [
    "### Pretraining Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d382337",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17756\\1843382543.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpretrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Data/Model3/pretraining_data_raw.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    579\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    582\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1251\u001b[0m             \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1252\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1253\u001b[1;33m                 \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1254\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 225\u001b[1;33m                 \u001b[0mchunks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    226\u001b[0m                 \u001b[1;31m# destructive to chunks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\dtypes\\common.py\u001b[0m in \u001b[0;36mis_extension_array_dtype\u001b[1;34m(arr_or_dtype)\u001b[0m\n\u001b[0;32m   1427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1429\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mis_extension_array_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1430\u001b[0m     \"\"\"\n\u001b[0;32m   1431\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0man\u001b[0m \u001b[0mobject\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0mextension\u001b[0m \u001b[0marray\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pretrain_data = pd.read_csv('Data/Model3/pretraining_data_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c2806a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_data = pretrain_data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c425e0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pt = pretrain_data.iloc[:, :-1]\n",
    "y_pt = pretrain_data.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa4a80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d689577c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_components = 200\n",
    "# pca = PCA(n_components=n_components)\n",
    "# pca.fit(X_pt)\n",
    "# X_pt = pca.transform(X_pt)\n",
    "# X_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e5c7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_pt = pd.DataFrame(X_pt, columns=[f'PC{i+1}' for i in range(X_pt.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0b1fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa158f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_pt = X_pt.drop(columns=['crew_stay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d91e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "crew_columns = ['crew_up', 'crew_down', 'crew_left', 'crew_right']\n",
    "X_pt['sum_crew'] = X_pt[crew_columns].sum(axis=1)\n",
    "\n",
    "for col in crew_columns:\n",
    "    X_pt.loc[X_pt['sum_crew'] != 0, col] = X_pt[col] / X_pt['sum_crew']\n",
    "\n",
    "X_pt.drop('sum_crew', axis=1, inplace=True)\n",
    "\n",
    "X_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4344a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pt = y_pt.apply(ast.literal_eval)\n",
    "y_pt = y_pt.apply(lambda x: x.index(1))\n",
    "y_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d38cbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pt_train, X_pt_test, y_pt_train, y_pt_test = train_test_split(X_pt, y_pt, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b430ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pt_train_tensor = torch.tensor(X_pt_train.values, dtype=torch.float32)\n",
    "y_pt_train_tensor = torch.tensor(y_pt_train.values, dtype=torch.long)\n",
    "\n",
    "X_pt_test_tensor = torch.tensor(X_pt_test.values, dtype=torch.float32)\n",
    "y_pt_test_tensor = torch.tensor(y_pt_test.values, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b69811",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pt_train_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08258b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pt_train_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a5fbba",
   "metadata": {},
   "source": [
    "### Initialize + Pretrain ACTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f089f97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = X_pt_train.shape[1]\n",
    "actor = Actor(num_features)\n",
    "optimizer = torch.optim.Adam(actor.parameters(), lr = 0.001) # Set learning rate to 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cd7317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actor_rand = Actor(num_features)\n",
    "# optimizer_rand = torch.optim.Adam(actor_rand.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490d4a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list = train_actor(actor, optimizer, X_pt_train_tensor, y_pt_train_tensor, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f3d5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a65060",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_actor(actor, X_pt_train_tensor, y_pt_train_tensor, X_pt_test_tensor, y_pt_test_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ba6b7b",
   "metadata": {},
   "source": [
    "### Generate Initial ACTOR Data for CRITIC Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c21a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Bot1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a70da48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name = 'Data/Model3/actor_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eb12cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Bot1.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea18896c",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid, open_cells = create_grid() # Fixed grid orientation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacd641e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid(x, y, move, grid, open_cells):\n",
    "    if move == 'up' and (x, y + 1) in open_cells:\n",
    "        return True\n",
    "    elif move == 'down' and (x, y - 1) in open_cells:\n",
    "        return True\n",
    "    elif move == 'left' and (x - 1, y) in open_cells:\n",
    "        return True\n",
    "    elif move == 'right' and (x + 1, y) in open_cells:\n",
    "        return True\n",
    "    elif move == 'stay':\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb04c04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_to_move(bot, prediction):\n",
    "    next_move = bot\n",
    "    \n",
    "    if prediction == 0:\n",
    "        next_move = (bot[0], bot[1] + 1)\n",
    "    elif prediction == 1:\n",
    "        next_move = (bot[0], bot[1] - 1)\n",
    "    elif prediction == 2:\n",
    "        next_move = (bot[0] - 1, bot[1])\n",
    "    elif prediction == 3:\n",
    "        next_move = (bot[0] + 1, bot[1])\n",
    "    else:\n",
    "        next_move = bot\n",
    "        \n",
    "    return next_move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb8e0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_valid_matrix(X):\n",
    "    global grid, open_cells\n",
    "    directions = ['up', 'down', 'left', 'right', 'stay']\n",
    "    valid_list = []\n",
    "    for i in range(len(X)):\n",
    "        x, y = X.iloc[i, 0], X.iloc[i, 1]\n",
    "        validity_for_each_direction = [is_valid(x, y, move, grid, open_cells) for move in directions]\n",
    "        valid_list.append(validity_for_each_direction)\n",
    "\n",
    "    valid_array = np.array(valid_list)\n",
    "    return valid_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0076f781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_probabilities(bot, matrix):\n",
    "    directions = {'up': (bot[0], bot[1] - 1), \n",
    "                  'down': (bot[0], bot[1] + 1), \n",
    "                  'left': (bot[0] - 1, bot[1]), \n",
    "                  'right': (bot[0] + 1, bot[1]),\n",
    "                  'stay': bot}\n",
    "    return [matrix.get(directions[direction], 0) for direction in ['up', 'down', 'left', 'right', 'stay']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0907ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_d_crew(ship, bot, alpha, d_lookup_table, crew_list, crew_matrix, open_cells):\n",
    "    directions = {'up': (bot[0], bot[1] - 1), \n",
    "                  'down': (bot[0], bot[1] + 1), \n",
    "                  'left': (bot[0] - 1, bot[1]), \n",
    "                  'right': (bot[0] + 1, bot[1]),\n",
    "                  'stay': bot}\n",
    "    \n",
    "    direction_values = set(directions.values())\n",
    "    \n",
    "    filtered_crew_matrix = {k: v for k, v in crew_matrix.items() if k not in direction_values}\n",
    "    \n",
    "    if filtered_crew_matrix:\n",
    "        max_crew_cell = max(filtered_crew_matrix, key=filtered_crew_matrix.get)\n",
    "    else:\n",
    "        return [0] * 5, d_lookup_table\n",
    "        \n",
    "    d_list = []\n",
    "    \n",
    "    for direction in ['up', 'down', 'left', 'right', 'stay']:\n",
    "        if (directions[direction] in open_cells or directions[direction] == bot) and directions[direction] not in crew_list:\n",
    "            _, d_lookup_table = crew_sensor(ship, directions[direction], alpha, d_lookup_table, crew_list)\n",
    "            d_dict = d_lookup_table.get(directions[direction])\n",
    "            d_list.append(1 / d_dict[max_crew_cell[0], max_crew_cell[1]])\n",
    "        else:\n",
    "            d_list.append(0)\n",
    "    \n",
    "    return d_list, d_lookup_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a347a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_d_alien(ship, bot, alpha, d_lookup_table, alien_list, crew_list, alien_matrix, open_cells):\n",
    "    directions = {'up': (bot[0], bot[1] - 1), \n",
    "                  'down': (bot[0], bot[1] + 1), \n",
    "                  'left': (bot[0] - 1, bot[1]), \n",
    "                  'right': (bot[0] + 1, bot[1]),\n",
    "                  'stay': bot}\n",
    "    \n",
    "    direction_values = set(directions.values())\n",
    "    \n",
    "    filtered_alien_matrix = {k: v for k, v in alien_matrix.items() if k not in direction_values}\n",
    "    \n",
    "    if filtered_alien_matrix:\n",
    "        max_alien_cell = max(filtered_alien_matrix, key=filtered_alien_matrix.get)\n",
    "    else:\n",
    "        return [0] * 5, d_lookup_table\n",
    "        \n",
    "    d_list = []\n",
    "    \n",
    "    for direction in ['up', 'down', 'left', 'right', 'stay']:\n",
    "        if (directions[direction] in open_cells or directions[direction] == bot) and directions[direction] not in crew_list and directions[direction] not in alien_list:\n",
    "            _, d_lookup_table = crew_sensor(ship, directions[direction], alpha, d_lookup_table, crew_list)\n",
    "            d_dict = d_lookup_table.get(directions[direction])\n",
    "            d_list.append(1 / d_dict[max_alien_cell[0], max_alien_cell[1]])\n",
    "        else:\n",
    "            d_list.append(0)\n",
    "    \n",
    "    return d_list, d_lookup_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bd69b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_params(actor, bot, alien_matrix, crew_matrix, d_crew, d_alien, alien_detected, crew_detected):\n",
    "    alien_probs = determine_probabilities(bot, alien_matrix)\n",
    "    crew_probs = determine_probabilities(bot, crew_matrix)\n",
    "    \n",
    "    X = pd.DataFrame([{\n",
    "        'bot_x': bot[0],\n",
    "        'bot_y': bot[1],\n",
    "            \n",
    "        'alien_up': alien_probs[0],\n",
    "        'alien_down': alien_probs[1],\n",
    "        'alien_left': alien_probs[2],\n",
    "        'alien_right': alien_probs[3],\n",
    "        'alien_stay': alien_probs[4],\n",
    "            \n",
    "        'crew_up': crew_probs[0],\n",
    "        'crew_down': crew_probs[1],\n",
    "        'crew_left': crew_probs[2],\n",
    "        'crew_right': crew_probs[3],\n",
    "            \n",
    "        'd_crew_up': np.float32(d_crew[0]),\n",
    "        'd_crew_down': np.float32(d_crew[1]),\n",
    "        'd_crew_left': np.float32(d_crew[2]),\n",
    "        'd_crew_right': np.float32(d_crew[3]),\n",
    "        'd_crew_stay': np.float32(d_crew[4]),\n",
    "        \n",
    "#         'd_alien_up': np.float32(d_alien[0]),\n",
    "#         'd_alien_down': np.float32(d_alien[1]),\n",
    "#         'd_alien_left': np.float32(d_alien[2]),\n",
    "#         'd_alien_right': np.float32(d_alien[3]),\n",
    "#         'd_alien_stay': np.float32(d_alien[4]),\n",
    "            \n",
    "        'alien_detected': 1 if alien_detected else 0,\n",
    "        'crew_detected': 1 if crew_detected else 0,\n",
    "    }])\n",
    "    \n",
    "#     X = pd.DataFrame([{\n",
    "#         'bot_x': bot[0],\n",
    "#         'bot_y': bot[1],\n",
    "\n",
    "#         'alien_up': alien_probs[0],\n",
    "#         'alien_down': alien_probs[1],\n",
    "#         'alien_left': alien_probs[2],\n",
    "#         'alien_right': alien_probs[3],\n",
    "#         'alien_stay': alien_probs[4],\n",
    "\n",
    "#         'crew_up': crew_probs[0],\n",
    "#         'crew_down': crew_probs[1],\n",
    "#         'crew_left': crew_probs[2],\n",
    "#         'crew_right': crew_probs[3],\n",
    "\n",
    "#         'alien_detected': 1 if alien_detected else 0,\n",
    "#         'crew_detected': 1 if crew_detected else 0,\n",
    "#     }], columns=['bot_x', 'bot_y', 'alien_up', 'alien_down', 'alien_left', 'alien_right', 'alien_stay', 'crew_up', 'crew_down', 'crew_left', 'crew_right', 'alien_detected', 'crew_detected'])\n",
    "    \n",
    "    valid = create_valid_matrix(X)\n",
    "    \n",
    "    crew_columns = ['crew_up', 'crew_down', 'crew_left', 'crew_right']\n",
    "    X['sum_crew'] = X[crew_columns].sum(axis=1)\n",
    "\n",
    "    for col in crew_columns:\n",
    "        X.loc[X['sum_crew'] != 0, col] = X[col] / X['sum_crew']\n",
    "\n",
    "    X.drop('sum_crew', axis=1, inplace=True)\n",
    "    \n",
    "    X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
    "    \n",
    "    prediction = predict_action(actor, X_tensor, valid)\n",
    "    next_move = predict_to_move(bot, prediction)\n",
    "    \n",
    "    return next_move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69c5c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Actor_Bot1(k, alpha, max_iter, timeout, actor, filename):\n",
    "    global grid, open_cells\n",
    "    \n",
    "    grid, open_cells = reset_grid(grid, open_cells)\n",
    "    bot, ship, open_cells = place_bot(grid, open_cells)\n",
    "\n",
    "    crew_list = []\n",
    "    alien_list = []\n",
    "    d_lookup_table = {}\n",
    "    \n",
    "    data_log = [] # Data Log Initialization\n",
    "\n",
    "    crew_list, ship = place_crew(ship, open_cells, crew_list)\n",
    "    alien_list, ship = place_alien(ship, open_cells, alien_list, bot, k)\n",
    "\n",
    "    alien_matrix = initialize_alienmatrix(open_cells, bot, k)\n",
    "    crew_matrix = initialize_crewmatrix(open_cells, crew_list, bot)\n",
    "    \n",
    "    alien_detected = alien_sensor(alien_list, bot, k) # Initially Run Alien Sensor\n",
    "    crew_detected, d_lookup_table = crew_sensor(ship, bot, alpha, d_lookup_table, crew_list) # Initially Run Crew Sensor\n",
    "    \n",
    "    win_count = 0\n",
    "    loss_count = 0\n",
    "    move = 0\n",
    "    win_move_count = []\n",
    "    marker = 0\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    cur_df = pd.DataFrame()\n",
    "    \n",
    "    while (win_count + loss_count) < max_iter:\n",
    "        neighbors = check_valid_neighbors(len(ship), bot[0], bot[1])\n",
    "        open_moves = [neigh for neigh in neighbors if (grid[neigh] != 1)]\n",
    "        open_moves.append(bot)\n",
    "        \n",
    "        # Data Collection Process\n",
    "        \n",
    "        alien_probs = determine_probabilities(bot, alien_matrix)\n",
    "        crew_probs = determine_probabilities(bot, crew_matrix)\n",
    "        d_crew, d_lookup_table = determine_d_crew(ship, bot, alpha, d_lookup_table, crew_list, crew_matrix, open_cells) # Find shortest distance from highest probability crew cell to all neighbors\n",
    "        d_alien, d_lookup_table = determine_d_alien(ship, bot, alpha, d_lookup_table, alien_list, crew_list, alien_matrix, open_cells) # Find shortest distance from highest probability alien cell to all neighbors\n",
    "        \n",
    "        next_move = predict_with_params(actor, bot, alien_matrix, crew_matrix, d_crew, d_alien, alien_detected, crew_detected) # Predict using trained network\n",
    "        \n",
    "        crew_probs.pop()\n",
    "        \n",
    "        if sum(crew_probs) > 0:\n",
    "            crew_probs = [c / sum(crew_probs) for c in crew_probs]\n",
    "        \n",
    "        # Convert relative move to string      \n",
    "        if next_move[0] > bot[0]:\n",
    "            next_move_str = 'right'\n",
    "        elif next_move[0] < bot[0]:\n",
    "            next_move_str = 'left'\n",
    "        elif next_move[1] > bot[1]:\n",
    "            next_move_str = 'up'\n",
    "        elif next_move[1] < bot[1]:\n",
    "            next_move_str = 'down'\n",
    "        else:\n",
    "            next_move_str = 'stay'\n",
    "        \n",
    "        actions = {'up': 0, 'down': 1, 'left': 2, 'right': 3, 'stay': 4}\n",
    "        best_move_encoded = actions[next_move_str]\n",
    "        \n",
    "        log_entry = {\n",
    "            'bot_x': bot[0],\n",
    "            'bot_y': bot[1],\n",
    "            \n",
    "            'alien_up': alien_probs[0],\n",
    "            'alien_down': alien_probs[1],\n",
    "            'alien_left': alien_probs[2],\n",
    "            'alien_right': alien_probs[3],\n",
    "            'alien_stay': alien_probs[4],\n",
    "            \n",
    "            'crew_up': crew_probs[0],\n",
    "            'crew_down': crew_probs[1],\n",
    "            'crew_left': crew_probs[2],\n",
    "            'crew_right': crew_probs[3],\n",
    "            \n",
    "            'd_crew_up': np.float32(d_crew[0]),\n",
    "            'd_crew_down': np.float32(d_crew[1]),\n",
    "            'd_crew_left': np.float32(d_crew[2]),\n",
    "            'd_crew_right': np.float32(d_crew[3]),\n",
    "            'd_crew_stay': np.float32(d_crew[4]),\n",
    "            \n",
    "#             'd_alien_up': np.float32(d_alien[0]),\n",
    "#             'd_alien_down': np.float32(d_alien[1]),\n",
    "#             'd_alien_left': np.float32(d_alien[2]),\n",
    "#             'd_alien_right': np.float32(d_alien[3]),\n",
    "#             'd_alien_stay': np.float32(d_alien[4]),\n",
    "            \n",
    "            'alien_detected': 1 if alien_detected else 0,\n",
    "            'crew_detected': 1 if crew_detected else 0,\n",
    "            \n",
    "            'chosen_action': best_move_encoded,\n",
    "            \n",
    "            'successful': 0\n",
    "        }\n",
    "#         data_log.append(log_entry)\n",
    "\n",
    "        cur_df = pd.concat([cur_df, pd.DataFrame([log_entry])], ignore_index=True)\n",
    "\n",
    "#         log_entry = {\n",
    "#             'bot_x': bot[0],\n",
    "#             'bot_y': bot[1],\n",
    "            \n",
    "#             'alien_up': alien_probs[0],\n",
    "#             'alien_down': alien_probs[1],\n",
    "#             'alien_left': alien_probs[2],\n",
    "#             'alien_right': alien_probs[3],\n",
    "#             'alien_stay': alien_probs[4],\n",
    "            \n",
    "#             'crew_up': crew_probs[0],\n",
    "#             'crew_down': crew_probs[1],\n",
    "#             'crew_left': crew_probs[2],\n",
    "#             'crew_right': crew_probs[3],\n",
    "            \n",
    "#             'alien_detected': 1 if alien_detected else 0,\n",
    "#             'crew_detected': 1 if crew_detected else 0,\n",
    "            \n",
    "#             'chosen_action': best_move_encoded\n",
    "#         }\n",
    "#         data_log.append(log_entry)\n",
    "        \n",
    "        prev_win_count = win_count\n",
    "        bot, crew_list, ship, open_cells, win_count, marker = move_bot(ship, bot, next_move, crew_list, alien_list, open_cells, win_count, 1)\n",
    "        move += 1\n",
    "\n",
    "        if marker == 1 or move >= timeout:\n",
    "            loss_count += 1\n",
    "            print(f\"ACTOR captured! Win Count: {win_count}, Loss Count: {loss_count}\")\n",
    "            \n",
    "            df = pd.concat([df, cur_df], ignore_index=True)\n",
    "            cur_df.drop(cur_df.index, axis=0, inplace=True)\n",
    "            cur_df.drop(cur_df.columns, axis=1, inplace=True)\n",
    "            \n",
    "            grid, open_cells = reset_grid(grid, open_cells)\n",
    "            bot, ship, open_cells = place_bot(grid, open_cells)\n",
    "            crew_list = []\n",
    "            alien_list = []\n",
    "            d_lookup_table = {}\n",
    "\n",
    "            crew_list, ship = place_crew(ship, open_cells, crew_list)\n",
    "            alien_list, ship = place_alien(ship, open_cells, alien_list, bot, k)\n",
    "\n",
    "            alien_matrix = initialize_alienmatrix(open_cells, bot, k)\n",
    "            crew_matrix = initialize_crewmatrix(open_cells, crew_list, bot)\n",
    "            marker = 0\n",
    "            move = 0\n",
    "\n",
    "            continue\n",
    "\n",
    "        if win_count > prev_win_count:\n",
    "            print(f\"Crew saved! Win Count: {win_count}, Loss Count: {loss_count}\")\n",
    "            \n",
    "            cur_df['successful'] = 1\n",
    "            df = pd.concat([df, cur_df], ignore_index=True)\n",
    "            cur_df.drop(cur_df.index, axis=0, inplace=True)\n",
    "            cur_df.drop(cur_df.columns, axis=1, inplace=True)\n",
    "            \n",
    "            win_move_count.append(move)\n",
    "            move = 0\n",
    "            d_lookup_table = {}\n",
    "            alien_matrix = initialize_alienmatrix(open_cells, bot, k)\n",
    "            crew_matrix = initialize_crewmatrix(open_cells, crew_list, bot)\n",
    "        \n",
    "        print(f\"ACTOR: {bot}, Crew: {crew_list}, Aliens: {alien_list}\")\n",
    "\n",
    "        alien_matrix, crew_matrix = update_afterbotmove(bot, alien_matrix, crew_matrix)\n",
    "\n",
    "        # Move bot to optimal neighbor\n",
    "        marker, alien_list, ship = move_aliens(ship, alien_list, bot) # Move alien randomly\n",
    "\n",
    "        if marker == 1 or move >= timeout:\n",
    "            loss_count += 1\n",
    "            print(f\"ACTOR captured! Win Count: {win_count}, Loss Count: {loss_count}\")\n",
    "\n",
    "            df = pd.concat([df, cur_df], ignore_index=True)\n",
    "            cur_df.drop(cur_df.index, axis=0, inplace=True)\n",
    "            cur_df.drop(cur_df.columns, axis=1, inplace=True)\n",
    "            \n",
    "            grid, open_cells = reset_grid(grid, open_cells)\n",
    "            bot, ship, open_cells = place_bot(grid, open_cells)\n",
    "            crew_list = []\n",
    "            alien_list = []\n",
    "            d_lookup_table = {}\n",
    "\n",
    "            crew_list, ship = place_crew(ship, open_cells, crew_list)\n",
    "            alien_list, ship = place_alien(ship, open_cells, alien_list, bot, k)\n",
    "\n",
    "            alien_matrix = initialize_alienmatrix(open_cells, bot, k)\n",
    "            crew_matrix = initialize_crewmatrix(open_cells, crew_list, bot)\n",
    "            marker = 0\n",
    "            move = 0\n",
    "\n",
    "            continue\n",
    "        \n",
    "        alien_matrix = update_afteralienmove(ship, alien_list, alien_matrix) # Update after alien move\n",
    "        \n",
    "        alien_detected = alien_sensor(alien_list, bot, k) # Run Alien Sensor\n",
    "        crew_detected, d_lookup_table = crew_sensor(ship, bot, alpha, d_lookup_table, crew_list) # Run Crew Sensor\n",
    "        \n",
    "        alien_matrix = update_alienmatrix(alien_matrix, alien_detected, bot, k) # Update based on alien sensor\n",
    "\n",
    "        crew_matrix = update_crewmatrix(crew_matrix, crew_detected, d_lookup_table, bot, alpha) # Update based on crew sensor\n",
    "    \n",
    "    df = pd.concat([df, cur_df], ignore_index=True)\n",
    "        \n",
    "#     df = pd.DataFrame(data_log)\n",
    "    df.to_csv(filename, mode='w', index=False, header=True)\n",
    "\n",
    "    return sum(win_move_count) // max(1, len(win_move_count)), (win_count / max(1, (win_count + loss_count))), win_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143a655d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Actor_Bot1_Simulation(alpha_values, k_values, max_iter, timeout, num_simulations, actor, filename):\n",
    "    avg_rescue_moves_mbot1 = {k: [] for k in k_values}\n",
    "    prob_crew_rescue_mbot1 = {k: [] for k in k_values}\n",
    "    avg_crew_saved_mbot1 = {k: [] for k in k_values}\n",
    "\n",
    "    for k in k_values:\n",
    "        for alpha in alpha_values:\n",
    "            total_metric1_mbot1, total_metric2_mbot1, total_metric3_mbot1 = 0, 0, 0\n",
    "            \n",
    "            for i in range(num_simulations):\n",
    "                metric1_mbot1, metric2_mbot1, metric3_mbot1 = Actor_Bot1(k, alpha, max_iter, timeout, actor, filename)\n",
    "\n",
    "                total_metric1_mbot1 += metric1_mbot1\n",
    "                total_metric2_mbot1 += metric2_mbot1\n",
    "                total_metric3_mbot1 += metric3_mbot1\n",
    "\n",
    "            avg_metric1_mbot1 = total_metric1_mbot1 / num_simulations\n",
    "            avg_metric2_mbot1 = total_metric2_mbot1 / num_simulations\n",
    "            avg_metric3_mbot1 = total_metric3_mbot1 / num_simulations\n",
    "\n",
    "            print(f\"ACTOR: k={k}, Alpha={alpha}\\nAverage Rescue Moves={avg_metric1_mbot1}\\nProbability of Crew Rescue={avg_metric2_mbot1}\\nAverage Crew Saved={avg_metric3_mbot1}\\n\")\n",
    "            \n",
    "            avg_rescue_moves_mbot1[k].append(avg_metric1_mbot1)\n",
    "            prob_crew_rescue_mbot1[k].append(avg_metric2_mbot1)\n",
    "            avg_crew_saved_mbot1[k].append(avg_metric3_mbot1)\n",
    "\n",
    "    return avg_rescue_moves_mbot1, prob_crew_rescue_mbot1, avg_crew_saved_mbot1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a102b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_simulation_actor(alpha_values, k_values, max_iter, timeout, num_simulations, actor, filename):\n",
    "    avg_rescue_moves_mbot1, prob_crew_rescue_mbot1, avg_crew_saved_mbot1 = Actor_Bot1_Simulation(alpha_values, k_values, max_iter, timeout, num_simulations, actor, filename)\n",
    "\n",
    "    prob_crew_rescue_mbot1 = {k: [round(prob, 3) for prob in probs] for k, probs in prob_crew_rescue_mbot1.items()}\n",
    "\n",
    "    print(f\"ACTOR:\\nAverage Rescue Moves = {avg_rescue_moves_mbot1}\\nProbability of Crew Rescue = {prob_crew_rescue_mbot1}\\nAverage Crew Saved = {avg_crew_saved_mbot1}\\n\")\n",
    "    \n",
    "    return avg_rescue_moves_mbot1, prob_crew_rescue_mbot1, avg_crew_saved_mbot1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c0d271",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_values = [0.004]\n",
    "k_values = [3]\n",
    "max_iter = 1\n",
    "timeout = 10000\n",
    "num_simulations = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626a3f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'Data/Model3/actor_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e35080",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric1_mbot1, metric2_mbot1, metric3_mbot1 = test_simulation_actor(alpha_values, k_values, max_iter, timeout, num_simulations, actor, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83844a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metric1_mbot1, metric2_mbot1, metric3_mbot1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae41073e",
   "metadata": {},
   "source": [
    "### CRITIC Loop Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9862e01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_critic(filename):\n",
    "    actor_data = pd.read_csv(filename)\n",
    "    \n",
    "    X = actor_data.iloc[:, :-1]\n",
    "    y = actor_data.iloc[:, -1]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "    \n",
    "    X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "\n",
    "    X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "    \n",
    "    return X, y, X_train, y_train, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cedf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_train_critic(X_train, X_train_tensor, y_train_tensor):\n",
    "    num_features = X_train.shape[1]\n",
    "#     print(X_train, X_train.shape[1])\n",
    "    critic = Critic(num_features)\n",
    "    optimizer = torch.optim.Adam(critic.parameters(), lr = 0.001)\n",
    "    loss_list = train_critic(critic, optimizer, X_train_tensor, y_train_tensor, epochs=1000)\n",
    "    return critic, loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69139403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_critic_dataset(critic, X):\n",
    "    valid = create_valid_matrix(X)\n",
    "    X_critic = []\n",
    "    X_actor = X.iloc[:, :-1]\n",
    "\n",
    "    for index, row in X_actor.iterrows():\n",
    "        valid_moves = valid[index]\n",
    "        best_action, highest_prob = None, 0\n",
    "\n",
    "        for move, is_valid in enumerate(valid_moves):\n",
    "            if is_valid:\n",
    "                modified_row = row.copy()\n",
    "                modified_row['chosen_action'] = move\n",
    "                prob_success = predict_prob_success(critic, torch.tensor(np.array([modified_row.values]), dtype=torch.float32))\n",
    "                if prob_success > highest_prob:\n",
    "                    highest_prob, best_action = prob_success, move\n",
    "\n",
    "        X_critic.append(list(row.values) + [best_action])\n",
    "\n",
    "    critic_data = pd.DataFrame(X_critic, columns = list(X_actor.columns) + ['chosen_action'])\n",
    "    critic_data.to_csv('Data/Model3/critic_data.csv', index=False)\n",
    "\n",
    "    return critic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe75ab84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def critic_loop():\n",
    "    actor_filename = 'Data/Model3/actor_data.csv' # Use data generated by ACTOR\n",
    "    \n",
    "    X, y, X_train, y_train, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor = preprocess_critic(actor_filename)\n",
    "    critic, loss_list = init_train_critic(X_train, X_train_tensor, y_train_tensor)\n",
    "    \n",
    "    plot_loss(loss_list)\n",
    "    \n",
    "    critic_data = generate_critic_dataset(critic, X)\n",
    "    \n",
    "    return critic, critic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba550fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic, critic_data = critic_loop()\n",
    "critic_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846ccd69",
   "metadata": {},
   "source": [
    "### ACTOR Loop Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c28c123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_actor(filename):\n",
    "    critic_data = pd.read_csv(filename)\n",
    "    \n",
    "    X = critic_data.iloc[:, :-1]\n",
    "    y = critic_data.iloc[:, -1]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "    \n",
    "    X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "\n",
    "    X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "    \n",
    "    return X, y, X_train, y_train, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccaf73ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_train_actor(X_train, X_train_tensor, y_train_tensor):\n",
    "    num_features = X_train.shape[1]\n",
    "    actor = Actor(num_features)\n",
    "    optimizer = torch.optim.Adam(actor.parameters(), lr = 0.001)\n",
    "    loss_list = train_actor(actor, optimizer, X_train_tensor, y_train_tensor, epochs=1000)\n",
    "    return actor, loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d095d97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_actor_dataset(alpha_values, k_values, max_iter, timeout, num_simulations, actor, filename):\n",
    "    metric1_mbot1, metric2_mbot1, metric3_mbot1 = test_simulation_actor(alpha_values, k_values, max_iter, timeout, num_simulations, actor, filename)\n",
    "\n",
    "    return metric1_mbot1, metric2_mbot1, metric3_mbot1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e355c8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def actor_loop():\n",
    "    critic_filename = 'Data/Model3/critic_data.csv'\n",
    "    \n",
    "    X, y, X_train, y_train, X_train_tensor, y_train_tensor, X_test_tensor, y_test_tensor = preprocess_actor(critic_filename)\n",
    "    actor, loss_list = init_train_actor(X_train, X_train_tensor, y_train_tensor)\n",
    "    \n",
    "    plot_loss(loss_list)\n",
    "    \n",
    "    alpha_values = [0.004]\n",
    "    k_values = [3]\n",
    "    max_iter = 1\n",
    "    timeout = 10000\n",
    "    num_simulations = 10\n",
    "    \n",
    "    actor_filename = 'Data/Model3/actor_data.csv'\n",
    "    \n",
    "    metric1_mbot1, metric2_mbot1, metric3_mbot1 = update_actor_dataset(alpha_values, k_values, max_iter, timeout, num_simulations, actor, actor_filename)\n",
    "    \n",
    "    return actor, metric1_mbot1, metric2_mbot1, metric3_mbot1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c753b327",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor, metric1_mbot1, metric2_mbot1, metric3_mbot1 = actor_loop()\n",
    "metric1_mbot1, metric2_mbot1, metric3_mbot1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e22cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforcement_learning(num_iter):\n",
    "    for i in range(num_iter):\n",
    "        critic, critic_data = critic_loop()\n",
    "        actor, metric1_mbot1, metric2_mbot1, metric3_mbot1 = actor_loop()\n",
    "        print(metric1_mbot1, metric2_mbot1, metric3_mbot1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aec1eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reinforcement_learning(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
